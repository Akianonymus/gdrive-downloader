#!/usr/bin/env sh
SELF_SOURCE="true"
_bytes_to_human(){
b_bytes_to_human="$(printf "%.0f\n" "${1:-0}")" s_bytes_to_human=0
d_bytes_to_human='' type_bytes_to_human=''
while [ "$b_bytes_to_human" -gt 1024 ];do
d_bytes_to_human="$(printf ".%02d" $((b_bytes_to_human%1024*100/1024)))"
b_bytes_to_human=$((b_bytes_to_human/1024))&&s_bytes_to_human=$((s_bytes_to_human+=1))
done
j=0&&for i in B KB MB GB TB PB EB YB ZB;do
j="$((j+=1))"&&[ "$((j-1))" = "$s_bytes_to_human" ]&&type_bytes_to_human="$i"&&break
continue
done
printf "%s\n" "$b_bytes_to_human$d_bytes_to_human $type_bytes_to_human"
}
_check_debug(){
_print_center_quiet(){ { [ $# = 3 ]&&printf "%s\n" "$2";}||{ printf "%s%s\n" "$2" "$3";};}
if [ -n "$DEBUG" ];then
set -x&&PS4='-> '
_print_center(){ { [ $# = 3 ]&&printf "%s\n" "$2";}||{ printf "%s%s\n" "$2" "$3";};}
_clear_line(){ :;}&&_newline(){ :;}
else
if [ -z "$QUIET" ];then
case "$TERM" in
xterm*|rxvt*|urxvt*|linux*|vt*)ansi_escapes="true"
esac
if [ -t 2 ]&&[ -n "$ansi_escapes" ];then
! COLUMNS="$(_get_columns_size)"||[ "${COLUMNS:-0}" -lt 45 ] 2>|/dev/null&&_print_center(){ { [ $# = 3 ]&&printf "%s\n" "[ $2 ]";}||{ printf "%s\n" "[ $2$3 ]";};}
EXTRA_LOG="_print_center" CURL_PROGRESS="-#"&&export CURL_PROGRESS EXTRA_LOG
else
_print_center(){ { [ $# = 3 ]&&printf "%s\n" "[ $2 ]";}||{ printf "%s\n" "[ $2$3 ]";};}
_clear_line(){ :;}
fi
_newline(){ printf "%b" "$1";}
else
_print_center(){ :;}&&_clear_line(){ :;}&&_newline(){ :;}
fi
set +x
fi
}
_check_internet(){
"$EXTRA_LOG" "justify" "Checking Internet Connection.." "-"
if ! _timeout 10 curl -Is google.com --compressed;then
_clear_line 1
"${QUIET:-_print_center}" "justify" "Error: Internet connection" " not available." "="
exit 1
fi
_clear_line 1
}
_clear_line(){
printf "\033[%sA\033[2K" "$1"
}
_display_time(){
t_display_time="$1" day_display_time="$((t_display_time/60/60/24))"
hr_display_time="$((t_display_time/60/60%24))" min_display_time="$((t_display_time/60%60))" sec_display_time="$((t_display_time%60))"
[ "$day_display_time" -gt 0 ]&&printf '%d days ' "$day_display_time"
[ "$hr_display_time" -gt 0 ]&&printf '%d hrs ' "$hr_display_time"
[ "$min_display_time" -gt 0 ]&&printf '%d minute(s) ' "$min_display_time"
[ "$day_display_time" -gt 0 ]||[ "$hr_display_time" -gt 0 ]||[ "$min_display_time" -gt 0 ]&&printf 'and '
printf '%d seconds\n' "$sec_display_time"
}
_get_columns_size(){
{ command -v bash 1>|/dev/null&&bash -c 'shopt -s checkwinsize && (: && :); printf "%s\n" "${COLUMNS}" 2>&1';}||{ command -v zsh 1>|/dev/null&&zsh -c 'printf "%s\n" "${COLUMNS}"';}||{ command -v stty 1>|/dev/null&&_tmp="$(stty size)"&&printf "%s\n" "${_tmp##* }";}||{ command -v tput 1>|/dev/null&&tput cols;}||return 1
}
_json_value(){
{ [ "$2" -gt 0 ] 2>|/dev/null&&no_of_lines_json_value="$2";}||:
{ [ "$3" -gt 0 ] 2>|/dev/null&&num_json_value="$3";}||{ ! [ "$3" = all ]&&num_json_value=1;}
_tmp="$(grep -o "\"$1\"\:.*" ${no_of_lines_json_value:+-m} $no_of_lines_json_value)"||return 1
printf "%s\n" "$_tmp"|sed -e 's/.*"'"$1""\"://" -e 's/[",]*$//' -e 's/["]*$//' -e 's/[,]*$//' -e "s/^ //" -e 's/^"//' -n -e "$num_json_value"p||:
return 0
}
_print_center(){
[ $# -lt 3 ]&&printf "Missing arguments\n"&&return 1
term_cols_print_center="$COLUMNS"
type_print_center="$1" filler_print_center=""
case "$type_print_center" in
normal)out_print_center="$2"&&symbol_print_center="$3";;
justify)if
[ $# = 3 ]
then
input1_print_center="$2" symbol_print_center="$3" to_print_print_center="" out_print_center=""
to_print_print_center="$((term_cols_print_center-5))"
{ [ "${#input1_print_center}" -gt "$to_print_print_center" ]&&out_print_center="[ $(printf "%.${to_print_print_center}s\n" "$input1_print_center")..]";}||{ out_print_center="[ $input1_print_center ]";}
else
input1_print_center="$2" input2_print_center="$3" symbol_print_center="$4" to_print_print_center="" temp_print_center="" out_print_center=""
to_print_print_center="$((term_cols_print_center*47/100))"
{ [ "${#input1_print_center}" -gt "$to_print_print_center" ]&&temp_print_center=" $(printf "%.${to_print_print_center}s\n" "$input1_print_center")..";}||{ temp_print_center=" $input1_print_center";}
to_print_print_center="$((term_cols_print_center*46/100))"
{ [ "${#input2_print_center}" -gt "$to_print_print_center" ]&&temp_print_center="$temp_print_center$(printf "%.${to_print_print_center}s\n" "$input2_print_center").. ";}||{ temp_print_center="$temp_print_center$input2_print_center ";}
out_print_center="[$temp_print_center]"
fi
;;
*)return 1
esac
str_len_print_center="${#out_print_center}"
[ "$str_len_print_center" -ge "$((term_cols_print_center-1))" ]&&{
printf "%s\n" "$out_print_center"&&return 0
}
filler_print_center_len="$(((term_cols_print_center-str_len_print_center)/2))"
i_print_center=1&&while [ "$i_print_center" -le "$filler_print_center_len" ];do
filler_print_center="$filler_print_center$symbol_print_center"&&i_print_center="$((i_print_center+1))"
done
printf "%s%s%s" "$filler_print_center" "$out_print_center" "$filler_print_center"
[ "$(((term_cols_print_center-str_len_print_center)%2))" -ne 0 ]&&printf "%s" "$symbol_print_center"
printf "\n"
return 0
}
_timeout(){
timeout_timeout="${1:?Error: Specify Timeout}"&&shift
{
"$@"&
child="$!"
trap -- "" TERM
{
sleep "$timeout_timeout"
kill -9 "$child"
}&
wait "$child"
} 2>|/dev/null 1>&2
}
_download_file(){
[ $# -lt 3 ]&&printf "Missing arguments\n"&&return 1
file_id_download_file="$1" name_download_file="$2" server_size_download_file="$3" parallel_download_file="$4"
unset status_download_file old_status_download_file left_download_file downloaded_download_file
server_size_readable_download_file="$(_bytes_to_human "$server_size_download_file")"
_print_center "justify" "$name_download_file" " | ${server_size_download_file:+$server_size_readable_download_file}" "="
if [ -s "$name_download_file" ];then
local_size_download_file="$(wc -c <"$name_download_file")"
if [ "$local_size_download_file" -ge "$server_size_download_file" ];then
"${QUIET:-_print_center}" "justify" "File already present" "="&&_newline "\n"
_log_in_file
return 0
else
_print_center "justify" "File is partially" " present, resuming.." "-"
CONTINUE=" -C - "
fi
else
_print_center "justify" "Downloading file.." "-"
fi
"$EXTRA_LOG" "justify" "Fetching" " cookies.." "-"
curl -c "$TMPFILE"COOKIE -I $CURL_PROGRESS -o /dev/null "https://drive.google.com/uc?export=download&id=$file_id_download_file"||:
for _ in 1 2;do _clear_line 1;done
confirm_string="$(_tmp="$(grep -F 'download_warning' "$TMPFILE"COOKIE)"&&printf "%s\n" "${_tmp##*$(printf '\t')}")"||:
curl -L -s $CONTINUE $CURL_SPEED -b "$TMPFILE"COOKIE -o "$name_download_file" \
"https://drive.google.com/uc?export=download&id=$file_id_download_file${confirm_string:+&confirm=$confirm_string}" 2>|/dev/null 1>&2&
pid="$!"
if [ -n "$parallel_download_file" ];then
wait "$pid" 2>|/dev/null 1>&2
else
until [ -f "$name_download_file" ]&&[ -n "$pid" ];do sleep 0.5;done
until ! kill -0 "$pid" 2>|/dev/null 1>&2;do
downloaded_download_file="$(wc -c <"$name_download_file")"
status_download_file="$(_bytes_to_human "$downloaded_download_file")"
left_download_file="$(_bytes_to_human "$((server_size_download_file-downloaded_download_file))")"
sleep 0.5
if [ "$status_download_file" != "$old_status_download_file" ];then
printf '%s\r' "$(_print_center "justify" "Downloaded: $status_download_file" " | Left: $left_download_file" "=")"
fi
old_status_download_file="$status_download_file"
done
_newline "\n"
fi
if [ "$(wc -c <"$name_download_file")" -ge "$server_size_download_file" ];then
for _ in 1 2;do _clear_line 1;done
"${QUIET:-_print_center}" "justify" "Downloaded" "="&&_newline "\n"
else
"${QUIET:-_print_center}" "justify" "Error: Incomplete" " download." "=" 1>&2
return 1
fi
_log_in_file "$name_download_file" "$server_size_readable_download_file" "$file_id_download_file"
return 0
}
_download_file_main(){
[ $# -lt 2 ]&&printf "Missing arguments\n"&&return 1
unset line_download_file_main fileid_download_file_main name_download_file_main size_download_file_main parallel_download_file_main RETURN_STATUS&&retry_download_file_main="${RETRY:-0}"
[ "$1" = parse ]&&parallel_download_file_main="$3" line_download_file_main="$2" fileid_download_file_main="${line_download_file_main%%"|:_//_:|"*}" \
name_download_file_main="${line_download_file_main##*"|:_//_:|"}" size_download_file_main="$(_tmp="${line_download_file_main#*"|:_//_:|"}"&&printf "%s\n" "${_tmp%"|:_//_:|"*}")"
parallel_download_file_main="${parallel_download_file_main:-$5}"
unset RETURN_STATUS&&until [ "$retry_download_file_main" -le 0 ]&&[ -n "$RETURN_STATUS" ];do
if [ -n "$parallel_download_file_main" ];then
_download_file "${fileid_download_file_main:-$2}" "${name_download_file_main:-$3}" "${size_download_file_main:-$4}" true 2>|/dev/null 1>&2&&RETURN_STATUS=1&&break
else
_download_file "${fileid_download_file_main:-$2}" "${name_download_file_main:-$3}" "${size_download_file_main:-$4}"&&RETURN_STATUS=1&&break
fi
RETURN_STATUS=2 retry_download_file_main="$((retry_download_file_main-1))"&&continue
done
{ [ "$RETURN_STATUS" = 1 ]&&printf "%b" "${parallel_download_file_main:+$RETURN_STATUS\n}";}||printf "%b" "${parallel_download_file_main:+$RETURN_STATUS\n}" 1>&2
return 0
}
_download_folder(){
[ $# = 0 ]&&printf "Missing arguments\n"&&return 1
folder_id_download_folder="$1" name_download_folder="$2" parallel_download_folder="$3"
unset info_download_folder error_status_download_folder success_status_download_folder \
files_download_folder folders_download_folder files_size_download_folder files_name_download_folder folders_name_download_folder \
num_of_files_download_folder num_of_folders_download_folder
_newline "\n"
"$EXTRA_LOG" "justify" "$name_download_folder" "="
"$EXTRA_LOG" "justify" "Fetching folder" " details.." "-"
if ! info_download_folder="$(_api_request "files?q=%27$folder_id_download_folder%27+in+parents&fields=files(name,size,id,mimeType)")";then
"${QUIET:-_print_center}" "justify" "Error: Cannot" ", fetch folder details." "="
printf "%s\n" "$info_download_folder"&&return 1
fi&&_clear_line 1
"$EXTRA_LOG" "justify" "Preparing files list.." "="
files_download_folder="$(printf "%s\n" "$info_download_folder"|grep '"size":' -B3|_json_value id all all)"||:
files_size_download_folder="$(printf "%s\n" "$info_download_folder"|_json_value size all all)"||:
files_name_download_folder="$(printf "%s\n" "$info_download_folder"|grep size -B2|_json_value name all all)"||:
exec 5<<EOF
$(printf "%s\n" "$files_download_folder")
EOF
exec 6<<EOF
$(printf "%s\n" "$files_size_download_folder")
EOF
exec 7<<EOF
$(printf "%s\n" "$files_name_download_folder")
EOF
files_list_download_folder="$(while read -r id <&5&&read -r size <&6&&read -r name <&7;do
printf "%s\n" "$id|:_//_:|$size|:_//_:|$name"
done)"
exec 5<&-&&exec 6<&-&&exec 7<&-
_clear_line 1
"$EXTRA_LOG" "justify" "Preparing sub folders list.." "="
folders_download_folder="$(printf "%s\n" "$info_download_folder"|grep '"mimeType":.*folder.*' -B2|_json_value id all all)"||:
folders_name_download_folder="$(printf "%s\n" "$info_download_folder"|grep '"mimeType":.*folder.*' -B1|_json_value name all all)"||:
exec 5<<EOF
$(printf "%s\n" "$folders_download_folder")
EOF
exec 6<<EOF
$(printf "%s\n" "$folders_name_download_folder")
EOF
folders_list_download_folder="$(while read -r id <&5&&read -r name <&6;do
printf "%s\n" "$id|:_//_:|$name"
done)"
exec 5<&-&&exec 6<&-
_clear_line 1
if [ -z "${files_download_folder:-$folders_download_folder}" ];then
for _ in 1 2;do _clear_line 1;done&&_print_center "justify" "$name_download_folder" " | Empty Folder" "="&&_newline "\n"&&return 0
fi
[ -n "$files_download_folder" ]&&num_of_files_download_folder="$(($(printf "%s\n" "$files_download_folder"|wc -l)))"
[ -n "$folders_download_folder" ]&&num_of_folders_download_folder="$(($(printf "%s\n" "$folders_download_folder"|wc -l)))"
for _ in 1 2;do _clear_line 1;done
_print_center "justify" \
"$name_download_folder" \
"${num_of_files_download_folder:+ | $num_of_files_download_folder files}${num_of_folders_download_folder:+ | $num_of_folders_download_folder sub folders}" "="&&_newline "\n\n"
if [ -f "$name_download_folder" ];then
name_download_folder="$name_download_folder$(date +'%s')"
fi&&mkdir -p "$name_download_folder"
cd "$name_download_folder"||exit 1
if [ -n "$num_of_files_download_folder" ];then
if [ -n "$parallel_download_folder" ];then
NO_OF_PARALLEL_JOBS_FINAL="$((NO_OF_PARALLEL_JOBS>num_of_files_download_folder?num_of_files_download_folder:NO_OF_PARALLEL_JOBS))"
[ -f "$TMPFILE"SUCCESS ]&&rm "$TMPFILE"SUCCESS
[ -f "$TMPFILE"ERROR ]&&rm "$TMPFILE"ERROR
(printf "%s\n" "$files_download_folder"|xargs -n1 -P"$NO_OF_PARALLEL_JOBS_FINAL" -i sh -c '
                eval "${SOURCE_UTILS}"
                _download_file_main parse "{}" true
                ' 1>|"$TMPFILE"SUCCESS 2>|"$TMPFILE"ERROR)&
pid="$!"
until [ -f "$TMPFILE"SUCCESS ]||[ -f "$TMPFILE"ERROR ];do sleep 0.5;done
_clear_line 1
until ! kill -0 "$pid" 2>|/dev/null 1>&2;do
success_status_download_folder="$(($(wc -l <"$TMPFILE"SUCCESS)))"
error_status_download_folder="$(($(wc -l <"$TMPFILE"ERROR)))"
sleep 1
if [ "$((success_status_download_folder+error_status_download_folder))" != "$TOTAL" ];then
printf '%s\r' "$(_print_center "justify" "Status" ": ${success_status_download_folder:-0} Downloaded | ${error_status_download_folder:-0} Failed" "=")"
fi
TOTAL="$((success_status_download_folder+error_status_download_folder))"
done
_newline "\n"
success_status_download_folder="$(($(wc -l <"$TMPFILE"SUCCESS)))"
error_status_download_folder="$(($(wc -l <"$TMPFILE"ERROR)))"
_clear_line 1&&_newline "\n"
else
while read -r line <&4&&{ [ -n "$line" ]||continue;};do
_download_file_main parse "$line"
: "$((RETURN_STATUS<2?(success_status_download_folder+=1):(error_status_download_folder+=1)))"
if [ -z "$VERBOSE" ];then
for _ in 1 2 3 4;do _clear_line 1;done
fi
_print_center "justify" "Status" ": ${success_status_download_folder:-0} Downloaded | ${error_status_download_folder:-0} Failed" "="
done 4<<EOF
$(printf "%s\n" "$files_list_download_folder")
EOF
fi
fi
for _ in 1 2;do _clear_line 1;done
[ "$success_status_download_folder" -gt 0 ]&&"${QUIET:-_print_center}" "justify" "Downloaded" ": $success_status_download_folder" "="
[ "${error_status_download_folder:-0}" -gt 0 ]&&"${QUIET:-_print_center}" "justify" "Failed" ": $error_status_download_folder" "="
_newline "\n"
if [ -z "$SKIP_SUBDIRS" ]&&[ -n "$num_of_folders_download_folder" ];then
while read -r line <&4&&{ [ -n "$line" ]||continue;};do
(_download_folder "${line%%"|:_//_:|"*}" "${line##*"|:_//_:|"}" "${parallel:-}")
done 4<<EOF
$(printf "%s\n" "$folders_list_download_folder")
EOF
fi
return 0
}
_log_in_file(){
[ -z "$LOG_FILE_ID" ]||[ -d "$LOG_FILE_ID" ]&&return 0
{
printf "%s\n" "Name: $1"
printf "%s\n" "Size: $2"
printf "%s\n\n" "ID: $3"
} >>"$LOG_FILE_ID"
}
_api_request(){
curl -e "https://drive.google.com" --compressed $CURL_PROGRESS \
"$API_URL/drive/$API_VERSION/${1:?}&key=$API_KEY&supportsAllDrives=true&includeItemsFromAllDrives=true"||return 1
_clear_line 1 1>&2
}
_check_id(){
[ $# = 0 ]&&printf "Missing arguments\n"&&return 1
"$EXTRA_LOG" "justify" "Validating URL/ID.." "-"
id_check_id="$1" json_check_id=""
if json_check_id="$(_api_request "files/$id_check_id?alt=json&fields=name,size,mimeType")";then
if ! printf "%s\n" "$json_check_id"|_json_value code 1 1 2>|/dev/null 1>&2;then
NAME="$(printf "%s\n" "$json_check_id"|_json_value name 1 1||:)"
mime_check_id="$(printf "%s\n" "$json_check_id"|_json_value mimeType 1 1||:)"
_clear_line 1
case "$mime_check_id" in
*folder*)FOLDER_ID="$id"
_print_center "justify" "Folder Detected" "="&&_newline "\n"
;;
*)SIZE="$(printf "%s\n" "$json_check_id"|_json_value size 1 1||:)"
FILE_ID="$id"
_print_center "justify" "File Detected" "="&&_newline "\n"
esac
else
_clear_line 1&&"${QUIET:-_print_center}" "justify" "Invalid URL/ID" "="&&_newline "\n"
return 1
fi
else
_clear_line 1
"${QUIET:-_print_center}" "justify" "Error: Cannot check URL/ID" "="
printf "%s\n" "$json_check_id"
return 1
fi
return 0
}
_extract_id(){
[ $# = 0 ]&&printf "Missing arguments\n"&&return 1
LC_ALL=C id_extract_id="$1"
case "$id_extract_id" in
*'drive.google.com'*'id='*)_tmp="${id_extract_id##*id=}"&&_tmp="${_tmp%%\?*}"&&id_extract_id="${_tmp%%\&*}";;
*'drive.google.com'*'file/d/'*|'http'*'docs.google.com'*'/d/'*)_tmp="${id_extract_id##*\/d\/}"&&_tmp="${_tmp%%\/*}"&&_tmp="${_tmp%%\?*}"&&id_extract_id="${_tmp%%\&*}";;
*'drive.google.com'*'drive'*'folders'*)_tmp="${id_extract_id##*\/folders\/}"&&_tmp="${_tmp%%\?*}"&&id_extract_id="${_tmp%%\&*}"
esac
printf "%b" "${id_extract_id:+$id_extract_id\n}"
}
_usage(){
printf "%b" "
The script can be used to download file/directory from google drive.\n
Usage:\n ${0##*/} [options.. ] <file_[url|id]> or <folder[url|id]>\n
Options:\n
  -d | --directory 'foldername' - option to _download given input in custom directory.\n
  -s | --skip-subdirs - Skip downloading of sub folders present in case of folders.\n
  -p | --parallel 'no_of_files_to_parallely_upload' - Download multiple files in parallel.\n
  --speed 'speed' - Limit the download speed, supported formats: 1K, 1M and 1G.\n
  -R | --retry 'num of retries' - Retry the file upload if it fails, postive integer as argument. Currently only for file uploads.\n
  -l | --log 'file_to_save_info' - Save downloaded files info to the given filename.\n
  -q | --quiet - Supress the normal output, only show success/error upload messages for files, and one extra line at the beginning for folder showing no. of files and sub folders.\n
  -V | --verbose - Display detailed message (only for non-parallel uploads).\n
  --skip-internet-check - Do not check for internet connection, recommended to use in sync jobs.\n
  -u | --update - Update the installed script in your system.\n
  --info - Show detailed info, only if script is installed system wide.\n
  --uninstall - Uninstall script, remove related files.\n
  -D | --debug - Display script command trace.\n
  -h | --help - Display usage instructions.\n"
exit 0
}
_short_help(){
printf "No valid arguments provided, use -h/--help flag to see usage.\n"
exit 0
}
_auto_update(){
export REPO
(_REPO="$REPO"
command -v "$COMMAND_NAME" 1>/dev/null&&[ -n "${_REPO:+${COMMAND_NAME:+${INSTALL_PATH:+${TYPE:+$TYPE_VALUE}}}}" ]&&[ "$((LAST_UPDATE_TIME+AUTO_UPDATE_INTERVAL))" -lt "$(date +'%s')" ]&&_update 2>|/dev/null 1>&2) \
2>|/dev/null 1>&2&
return 0
}
_update(){
job_update="${1:-update}"
[ "$GLOBAL_INSTALL" = true ]&&! [ "$(id -u)" = 0 ]&&printf "%s\n" "Error: Need root access to update."&&return 0
[ "$job_update" = uninstall ]&&job_string_update="--uninstall"
_print_center "justify" "Fetching $job_update script.." "-"
repo_update="${REPO:-labbots/gdrive-downloader}" type_value_update="${TYPE_VALUE:-master}" cmd_update="${COMMAND_NAME:-gdl}" path_update="${INSTALL_PATH:-$HOME/.gdrive-downloader}"
if script_update="$(curl --compressed -Ls "https://github.com/$repo_update/raw/$type_value_update/install.sh")";then
_clear_line 1
printf "%s\n" "$script_update"|sh -s -- ${job_string_update:-} --skip-internet-check --cmd "$cmd_update" --path "$path_update"
else
_clear_line 1
"${QUIET:-_print_center}" "justify" "Error: Cannot download" " $job_update script." "=" 1>&2
exit 1
fi
exit "$?"
}
_info(){
if command -v "$COMMAND_NAME" 1>/dev/null&&[ -n "${REPO:+${COMMAND_NAME:+${INSTALL_PATH:+${TYPE:+$TYPE_VALUE}}}}" ];then
for i in REPO INSTALL_PATH INSTALLATION TYPE TYPE_VALUE LATEST_INSTALLED_SHA;do
printf "%s\n" "$i=\"$(eval printf "%s" \"\$"$i"\")\""
done
else
printf "%s\n" "gdrive-downloader is not installed system wide."
fi
exit 0
}
_setup_arguments(){
[ $# = 0 ]&&printf "Missing arguments\n"&&return 1
unset LOG_FILE_ID FOLDERNAME SKIP_SUBDIRS NO_OF_PARALLEL_JOBS PARALLEL_DOWNLOAD
unset DEBUG QUIET VERBOSE VERBOSE_PROGRESS SKIP_INTERNET_CHECK RETRY
unset ID_INPUT_ARRAY FINAL_INPUT_ARRAY
CURL_PROGRESS="-s" EXTRA_LOG=":"
API_KEY="AIzaSyD2dHsZJ9b4OXuy5B_owiL8W18NaNOM8tk"
API_URL="https://www.googleapis.com"
API_VERSION="v3"
_check_longoptions(){
[ -z "$2" ]&&printf '%s: %s: option requires an argument\nTry '"%s -h/--help"' for more information.\n' "${0##*/}" "$1" "${0##*/}"&&exit 1
return 0
}
while [ "$#" -gt 0 ];do
case "$1" in
-h|--help)_usage;;
-D|--debug)DEBUG="true"&&export DEBUG;;
-u|--update)_check_debug&&_update;;
-U|--uninstall)_check_debug&&_update uninstall;;
--info)_info;;
-l|--log)_check_longoptions "$1" "$2"
LOG_FILE_ID="$2"&&shift
;;
-d|--directory)_check_longoptions "$1" "$2"
FOLDERNAME="$2"&&shift
;;
-s|--skip-subdirs)SKIP_SUBDIRS="true"
;;
-p|--parallel)_check_longoptions "$1" "$2"
if [ "$2" -gt 0 ] 2>|/dev/null 1>&2;then
NO_OF_PARALLEL_JOBS="$2"
else
printf "\nError: -p/--parallel value ranges between 1 to 10.\n"
exit 1
fi
PARALLEL_DOWNLOAD="parallel"&&shift
;;
--speed)_check_longoptions "$1" "$2"
regex='^([0-9]+)([k,K]|[m,M]|[g,G])+$'
if printf "%s\n" "$2"|grep -qE "$regex";then
CURL_SPEED="--limit-rate $2"&&shift
else
printf "Error: Wrong speed limit format, supported formats: 1K , 1M and 1G\n" 1>&2
exit 1
fi
;;
-R|--retry)_check_longoptions "$1" "$2"
if [ "$((2))" -gt 0 ] 2>|/dev/null 1>&2;then
RETRY="$2"&&shift
else
printf "Error: -R/--retry only takes positive integers as arguments, min = 1, max = infinity.\n"
exit 1
fi
;;
-q|--quiet)QUIET="_print_center_quiet";;
-V|--verbose)VERBOSE="true"
;;
--skip-internet-check)SKIP_INTERNET_CHECK=":"
;;
*)if
[ -z "${1##-*}" ]
then
printf '%s: %s: Unknown option\nTry '"%s -h/--help"' for more information.\n' "${0##*/}" "$1" "${0##*/}"&&exit 1
else
ID_INPUT_ARRAY="$ID_INPUT_ARRAY
                                    $(_extract_id "$1")"
fi
esac
shift
done
[ -z "$ID_INPUT_ARRAY" ]&&_short_help
_check_debug
return 0
}
_process_arguments(){
export LOG_FILE_ID VERBOSE API_KEY API_URL API_VERSION \
FOLDERNAME SKIP_SUBDIRS NO_OF_PARALLEL_JOBS PARALLEL_DOWNLOAD SKIP_INTERNET_CHECK \
COLUMNS CURL_SPEED TMPFILE CURL_PROGRESS EXTRA_LOG RETRY QUIET SOURCE_UTILS
${FOLDERNAME:+mkdir -p $FOLDERNAME}
cd "${FOLDERNAME:-.}" 2>|/dev/null 1>&2||exit 1
unset Aseen&&while read -r id <&4&&{ [ -n "$id" ]||continue;}&&case "$Aseen" in
*"|:_//_:|$id|:_//_:|"*)continue;;
*)Aseen="$Aseen|:_//_:|$id|:_//_:|"
esac;do
_check_id "$id"||continue
if [ -n "$FOLDER_ID" ];then
_download_folder "$FOLDER_ID" "$NAME" "${PARALLEL_DOWNLOAD:-}"
else
_download_file_main noparse "$FILE_ID" "$NAME" "$SIZE"
fi
done 4<<EOF
$(printf "%s\n" "$ID_INPUT_ARRAY")
EOF
return 0
}
main(){
[ $# = 0 ]&&_short_help
if [ -z "$SELF_SOURCE" ];then
UTILS_FOLDER="${UTILS_FOLDER:-$PWD}"&&SOURCE_UTILS=". '$UTILS_FOLDER/common-utils.sh' && . '$UTILS_FOLDER/download-utils.sh' && . '$UTILS_FOLDER/drive-utils.sh'"
eval "$SOURCE_UTILS"||{ printf "Error: Unable to source util files.\n"&&exit 1;}
else
SOURCE_UTILS="SOURCED=true . \"$(cd "${0%\/*}"&&pwd)/${0##*\/}\""&&eval "$SOURCE_UTILS"
fi
set -o errexit -o noclobber
TMPFILE="$(command -v mktemp 1>|/dev/null&&mktemp -u)"||TMPFILE="$(pwd)/$(date +'%s').tmpfile"
export TMPFILE
_setup_arguments "$@"
"${SKIP_INTERNET_CHECK:-_check_internet}"
_cleanup(){
{
rm -f "${TMPFILE:?}"*
export abnormal_exit&&if [ -n "$abnormal_exit" ];then
printf "\n\n%s\n" "Script exited manually."
kill -9 -$$&
else
_auto_update
fi
} 2>|/dev/null||:
return 0
}
trap 'abnormal_exit="1" ; exit' INT TERM
trap '_cleanup' EXIT
START="$(date +'%s')"
_process_arguments
END="$(date +'%s')"
DIFF="$((END-START))"
"${QUIET:-_print_center}" "normal" " Time Elapsed: ""$((DIFF/60))"" minute(s) and ""$((DIFF%60))"" seconds. " "="
}
{ [ -z "$SOURCED" ]&&main "$@";}||:
